<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
  	equationNumbers: { autoNumber: "AMS" },
    Macros: {
        argmax: ["\\underset{#1}{\\operatorname{argmax}}", 1]
    }
  }
});
</script>
# 多変数ロジスティック分類
## 分類問題とは何か
典型的な分類の問題の例として以下が挙げられる

- 手書きの文字を認識する
- 罹患している病気を症状から推定する
- メールのスパムかどうかを判定する

このような問題をどのようにして確率的にモデリングするのか?
今回はそのような点について焦点をあてて考察していく

以下ではエントロピー最大化を第一原理としてロジスティック分類を導き、トレーニングデータが与えられたときの最適解の導出まで行う

## ロジスティック分布の導出

### 確率モデルで分類問題を表す

以下では一般的な形でモデリング化するため、入力の変数を $ \bf{x} \in \bf{R^D} $ として出力のラベルを $ C \in \{C_1, C_2, ..., C_K \} $  とする。今回は確率モデルとして定式化するため、ある入力が与えられたときの特定のラベルが出現する確率を考えればよい。

これは条件付き確率 $ P(C|\bf{x})$  として表せる。したがって、ある入力値 $ \bf{x} $ が与えられときの最適なラベルは、この条件付き確率を最大化させればよい。
この最適なラベルを $ C^{\star} $ とすれば以下のようにして

$$
\newcommand{\b}[1]{\mathbf{#1}}
\newcommand{\prob}[0]{P(C_k|\b{x^{(n)}})}
\newcommand{\probe}[0]{P_k^{(n)}}
\newcommand{\probeh}[0]{P_d^{(m)}}
\newcommand{\probw}[0]{\frac{\partial \probe}{\partial \probeh}}
\newcommand{\phix}[1]{\b{\phi(x^{(#1)}})}
C^{\star} = \argmax{k} P(C_k|\bf{x})
$$

として未知の入力値に対するラベルの予測を与えることができる。

以後は一般化も踏まえて、入力ベクトルをさらに固有の特徴を抽出するなんからの変換 $ \bf{\phi(x)} \in \bf{R^{K}} $ を加えたものを前提に考える。

### 最大エントロピー原理

さて、上記の議論はあくまで何らかのパラメーター $ \bf{\omega} $ を用いて上記の条件付き確率を制限して $ P(C|\bf{x}; \omega) $ のように表せないと、議論がこれ以上先に進めない。
そのため最大エントロピー原理を用いて、トレーニングデータが与えられときに、尤もらしい確率分布がどのように表せるかを考えたい。

以後は計算を簡単にするために、出力のラベルはすべて 1 of K 符号化で表されているとしよう。1 of K 符号化でラベルがエンコードされる場合、正解データが $ C_k $ とすると L 次元ベクトル $ \bf{t} \in \bf{R^{L}} $ として表せる。このとき $ t_i \in \{0, 1\} $ かつ $ t_i = \delta_{i,k} \, (i = 1, 2, ..., L) $ が成り立っている。
デルタ関数の定義は以下の通りである。

$$
\delta_{i,k} = \begin{cases}
	1 & (i = k) \\
	0 & (i \neq k) \\
\end{cases}
$$

定義がややこしそうだが、結局ラベルが L 個あったら L 次元ベクトルとして表し、C_k が正解データであれば k 番目の要素を 1 としてそれ以外を 0 とするベクトルである。

ここで、トレーニングデータとそのラベルをそれぞれ $ (\phix{1}, \bf{t^{(1)}}), (\phix{2}, \bf{t^{(2)}}),\, ...\, (\phix{N}, \bf{t^{(N)}}) $ が与えられたとする。

このとき

$$
\begin{align}
	\sum_{k = 1}^{K} \prob  & = & 1 \hspace{15pt} (n = 1, 2, ..., N) \label{eq:1.1} \\
	\sum_{n = 1}^{N} \prob \, \phix{n} & = & \sum_{n = 1}^{N} t_k^{(n)} \phix{n}  \hspace{15pt} (k = 1, 2, ..., K) \label {eq:1.2}
\end{align}
$$

が満たさなけれはならないと仮定しよう。

$ \eqref{eq:1.1} $ は確率の定義より明らかに満たさなければならない。
$ \eqref{eq:1.2} $ についてはいわゆる $ \prob $ が十分 $ t_k^{(n)} $ をよく表さなければならない、という制約である。条件付きエントロピーは $ - \sum_{k=1}^{L} \prob \log \prob $ より、これを $ \eqref{eq:1.1}, \eqref{eq:1.2} $ の制約の元で最大化すればよい。
$ \probe = \prob $ のように簡易的に表すことにすれば、ラグランジュの未定乗数法より

$$
\begin{equation}
	H(p) = \sum_{n = 1}^{N} \sum_{k = 1}^{K} - \probe \log \probe \\
		+ \sum_{n = 1}^{N} \lambda^{(n)} \{ \sum_{k = 1}^{K} \probe - 1 \} \\
		+ \sum_{k = 1}^{K} \b{\omega_k}^{t} \{\sum_{n = 1}^{N} \phix{n} (\probe - t_k^{(n)})\} \label{eq:1.3}
\end{equation}
$$

を最大にするような $ \probe $ を求めればよいことがわかる。
ここでスラッグ変数 $ \lambda^{(n)} $ と $ \b{\omega_k}^{t} $ を導入した。

### $ \probe $ を求める

式の定式化までは行えたのであとは $ H(p) $ を単純に $ \probeh $ で微分すればよい。

$$
\newcommand{\partialh}[0]{\frac{\partial H(p)}{\partial \probeh} }
\newcommand{\omegam}[1]{\b{\omega_{#1}^{t}} \phix{m}}
\newcommand{\expw}[1]{\exp \left \{ #1 \right \}}
\begin{aligned}
	\partialh & = \sum_{n, k} \left \{
		- \probw \{\log \probeh + 1\} +
		\lambda^{n} \{ \probw \} +
		\b{\omega_k^{t}} \phix{n} \probw
	\right \} \\
	& = - log \probeh - 1 + \lambda^{(m)} + \omegam{d}
\end{aligned}
$$

のように求まるので、以下のように微分をゼロをおけば

$$
\begin{align}
	\partialh & = 0 \\
	\probeh & = \exp \left \{
		\lambda^{(m)} - 1 + \omegam{d}
	\right \} \label{eq:1.4}
\end{align}
$$

$ \eqref{eq:1.4} $ を $ \eqref{eq:1.1} $ に代入すれば

$$
\begin{align}
	\expw{ \lambda^{(m)} - 1 } = \exp{(\omegam{d})} \label{eq:1.5}
\end{align}
$$

より $ \eqref{eq:1.5} $ を $ \eqref{eq:1.4} $ に代入して添字を整理すれば、

$$
\begin{align}
	\bbox[10pt, border: 2px dotted black]{
		\probe = \prob = \frac{
			\exp{(\omegam{k})}
		}{
			\sum_{d=1}^{K} \exp{(\omegam{d})}
		}
	} \label{eq:1.6}
\end{align}
$$

と表せる。
このようにして目的であった条件付き確率分布がパラメーター  $ \b{\omega_{d}^{t}} $ を用いて表せるところまで導出できた。

$ \eqref{eq:1.6} $ は多変数のロジスティック分布である



