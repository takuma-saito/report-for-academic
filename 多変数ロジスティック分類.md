<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
  	equationNumbers: { autoNumber: "AMS" },
    Macros: {
        argmax: ["\\underset{#1}{\\operatorname{argmax}}", 1]
    }
  }
});
</script>
# 多変数ロジスティック分類
## 分類問題とは何か
典型的な分類の問題の例として以下が挙げられる

- 手書きの文字を認識する
- 罹患している病気を症状から推定する
- メールのスパムかどうかを判定する

このような問題をどのようにして確率的にモデリングするのか?
今回はそのような点について焦点をあてて考察していく

以下ではエントロピー最大化を第一原理としてロジスティック分類を導き、トレーニングデータが与えられたときの最適解の導出まで行う

## ロジスティック分布の導出

### 確率モデルで分類問題を表す

以下では一般的な形でモデリング化するため、入力の変数を $ \bf{x} \in \bf{R^D} $ として出力のラベルを $ C \in \{C_1, C_2, ..., C_K \} $  とする。今回は確率モデルとして定式化するため、ある入力が与えられたときの特定のラベルが出現する確率を考えればよい。

これは条件付き確率 $ P(C|\bf{x})$  として表せる。したがって、ある入力値 $ \bf{x} $ が与えられときの最適なラベルは、この条件付き確率を最大化させるようなラベルである。
この最適なラベルを $ C^{\star} $ とすれば以下のようにして

$$
\newcommand{\b}[1]{\mathbf{#1}}
\newcommand{\prob}[0]{P(C_k|\b{x^{(n)}})}
\newcommand{\probe}[0]{P_k^{(n)}}
\newcommand{\probeh}[0]{P_d^{(m)}}
\newcommand{\probw}[0]{\frac{\partial \probe}{\partial \probeh}}
\newcommand{\phix}[1]{\b{\phi(x^{(#1)}})}
\begin{align}
	C^{\star} = \argmax{k} P(C_k|\bf{x}) \label{eq:0.1}
\end{align}
$$

未知の入力値に対するラベルの予測を与えることができる。

以後は一般化も踏まえて、入力ベクトルを固有の特徴を抽出する変換 $ \bf{\phi(x)} \in \bf{R^{K}} $ を加えたものを前提に考える。

### 最大エントロピー原理

さて、上記の議論はあくまで何らかのパラメーター $ \bf{\omega} $ を用いて上記の条件付き確率を制限して $ P(C|\bf{x}; \omega) $ のように表せないと、議論がこれ以上先に進めない。
そのため最大エントロピー原理を用いて、トレーニングデータが与えられときに、尤もらしい確率分布がどのように表せるかを考えたい。

以後は計算を簡単にするために、出力のラベルはすべて 1 of K 符号化で表されているとしよう。1 of K 符号化でラベルがエンコードされる場合、正解データが $ C_k $ とすると L 次元ベクトル $ \bf{t} \in \bf{R^{L}} $ として表せる。このとき $ t_i \in \{0, 1\} $ かつ $ t_i = \delta_{i,k} \, (i = 1, 2, ..., L) $ が成り立っている。
デルタ関数の定義は以下の通りである。

$$
\delta_{i,k} = \begin{cases}
	1 & (i = k) \\
	0 & (i \neq k) \\
\end{cases}
$$

定義がややこしそうだが、結局ラベルが L 個あったら L 次元ベクトルとして表し、C_k が正解データであれば k 番目の要素を 1 としてそれ以外を 0 とするベクトルである。

ここで、トレーニングデータとそのラベルをそれぞれ
$$
\begin{align}
	(\phix{1}, \bf{t^{(1)}}), (\phix{2}, \bf{t^{(2)}}),\, ...\, (\phix{N}, \bf{t^{(N)}}) \label{eq:1.0}
\end{align}
$$

が与えられたとする。
このとき

$$
\begin{align}
	\sum_{k = 1}^{K} \prob  & = 1 \hspace{15pt} (n = 1, 2, ..., N) \label{eq:1.1} \\
	\sum_{n = 1}^{N} \prob \, \phix{n} & = \sum_{n = 1}^{N} t_k^{(n)} \phix{n}  \hspace{15pt} (k = 1, 2, ..., K) \label {eq:1.2}
\end{align}
$$

が満たさなけれはならないと仮定しよう。

$ \eqref{eq:1.1} $ は確率の定義より明らかに満たさなければならない。
$ \eqref{eq:1.2} $ についてはいわゆる $ \prob $ が十分 $ t_k^{(n)} $ をよく表さなければならない、という制約である。条件付きエントロピーは $ - \sum_{k=1}^{L} \prob \ln \prob $ より、これを $ \eqref{eq:1.1}, \eqref{eq:1.2} $ の制約の元で最大化すればよい。
$ \probe = \prob $ のように簡易的に表すことにすれば、ラグランジュの未定乗数法より

$$
\begin{equation}
	H(p) = \sum_{n = 1}^{N} \sum_{k = 1}^{K} - \probe \ln \probe \\
		+ \sum_{n = 1}^{N} \lambda^{(n)} \{ \sum_{k = 1}^{K} \probe - 1 \} \\
		+ \sum_{k = 1}^{K} \b{\omega_k}^{t} \{\sum_{n = 1}^{N} \phix{n} (\probe - t_k^{(n)})\} \label{eq:1.3}
\end{equation}
$$

を最大にするような $ \probe $ を求めればよいことがわかる。
ここでスラッグ変数 $ \lambda^{(n)} $ と $ \b{\omega_k}^{t} $ を導入した。

### $ \probe $ を求める

式の定式化までは行えたのであとは $ H(p) $ を単純に $ \probeh $ で微分すればよい。

$$
\newcommand{\partialh}[0]{\frac{\partial H(p)}{\partial \probeh} }
\newcommand{\omegam}[1]{\b{\omega_{#1}^{t}} \phix{m}}
\newcommand{\expw}[1]{\exp \left \{ #1 \right \}}
\begin{aligned}
	\partialh & = \sum_{n, k} \left \{
		- \probw \{\ln \probeh + 1\} +
		\lambda^{n} \{ \probw \} +
		\b{\omega_k^{t}} \phix{n} \probw
	\right \} \\
	& = - ln \probeh - 1 + \lambda^{(m)} + \omegam{d}
\end{aligned}
$$

のように求まるので、以下のように微分をゼロをおけば

$$
\begin{align}
	\partialh & = 0 \\
	\probeh & = \exp \left \{
		\lambda^{(m)} - 1 + \omegam{d}
	\right \} \label{eq:1.4}
\end{align}
$$

$ \eqref{eq:1.4} $ を $ \eqref{eq:1.1} $ に代入すれば

$$
\begin{align}
	\expw{ \lambda^{(m)} - 1 } = \exp{(\omegam{d})} \label{eq:1.5}
\end{align}
$$

より $ \eqref{eq:1.5} $ を $ \eqref{eq:1.4} $ に代入して添字を整理すれば、

$$
\begin{align}
	\bbox[10pt, border: 2px dotted black]{
		\probe = \prob = \frac{
			\exp{(\omegam{k})}
		}{
			\sum_{d=1}^{K} \exp{(\omegam{d})}
		}
	} \label{eq:1.6}
\end{align}
$$

と表せる。
このようにして目的であった条件付き確率分布がパラメーター  $ \b{\omega_{d}^{t}} $ を用いて表せるところまで求めることができた

$ \eqref{eq:1.6} $ は多変数のロジスティック分布である

## 多変数ロジスティック分布の最尤推定
条件付き確率分布が得られたので $ \eqref{eq:1.0} $ のトレーニングデータが与えられたときに負の対数尤度は以下のように表せる。

$$
\newcommand{\bracket}[1]{\left \{ #1 \right \}}
\begin{align}
	H(\b{W}) & = - \ln \bracket{
		\prod_{n = 1}^{N} \prod_{k = 1}^{K} \prob^{t_k^{(n)}}
	} \\
	& = - \sum_{k, n} t_k^{(n)} {\rm ln} \,\probe \label{eq:2.1}
\end{align}
$$

この対数尤度を最大化するような $ \b{W} = \b{\omega_{k}^{t}} \hspace{5pt} (k = 1, 2, ... K) $ を最急勾配法によって表せればよい。

ただしここで

$$
\begin{align}
	\probe & = \frac{\exp \bracket{a_k^{(n)}}}{\sum_{d=1}^{K} \exp \bracket{a_d^{(n)}}} \\
\end{align}
$$

ただし

$$
\begin{aligned}
	a_k^{(n)} & = a_k(\b{x^{(n)}}) \\
	        & = \omegam{k} \\
	        & = \sum_{d = 1}^{D} \omega_{k,d} \phi_{d}(\b{x^{(n)}}) \\
	        & = \sum_{d = 1}^{D} \omega_{k,d} \phi_{d,n} \hspace{10pt} (\phi_{d}(\b{x^{(n)}}) = \phi_{d,n} とした)
\end{aligned}
$$

とする。
$ \eqref{eq:1.6} $ の関係式を変数の依存関係で分割しただけである。

このとき $ P_k $ に対して $ a_j $ の微分を考えると

$$
\newcommand{\expA}[1]{\exp \bracket{{#1}}}
\newcommand{\sumExpA}[0]{\sum_{d=1}^{K} \expA{a_d}}
\newcommand{\fracSumExpA}[0]{\frac{\expA{a_k}}{\sumExpA}}
\newcommand{\partialAj}[0]{\frac{\partial}{\partial a_j}}
\begin{aligned}
	\frac{\partial P_k}{\partial a_j}
		& = \partialAj \bracket{\fracSumExpA} \\
		& = \bracket{\partialAj (\expA{a_k})} \frac{1}{\sumExpA} + \\
			& \expA{a_k} (- \frac{1}{(\sumExpA)^2}) \partialAj \bracket{\sumExpA} \\
		& = \frac{\expA{a_k}}{(\sumExpA)} ( \delta_{jk} - \frac{\expA{a_j}}{(\sumExpA)} ) \\
		& = P_k (\delta_{k,j} - P_j)
\end{aligned}
$$

より

$$
\begin{align}
	\frac{\partial P_k}{\partial a_j} = P_k (\delta_{k,j} - P_j)
\end{align}
$$

が成り立つため $ \eqref{eq:2.1} $ を $ \omega_{m,j} $ に対して微分すると

$$
\begin{aligned}
	\frac{\partial H(\b{W})}{\partial \omega_{m,j}}
		& = 
			\sum_{n=1}^{N} \sum_{k=1}^{K} \sum_{l=1}^{K}
			t_k^{(n)} \bracket{
				\frac{\partial}{\partial a_l^{(n)}} ln P_k^{(n)}
			} \frac{\partial a_l^{(n)}}{\partial \omega_{m,j}} \\
		& = - \sum_{n, k, l} t_k^{(n)} P_k (\delta_{k,l} - P_j) \frac{\partial}{\partial \omega_{m,j}} \bracket{\sum_{d=1}^{D} \omega_{l,d} \phi_{d,n}} \\
		& = - \sum_{n, k, l} t_k^{(n)} P_k (\delta_{k,l} - P_j) \delta_{m,l} \phi_{j,n} \\
		& = \sum_{n=1}^{N} \bracket{
			P_m^{(n)} - t_m^{(n)}
		} \phi_{j,n}
\end{aligned}
$$

添字を差し替えて

$$
\begin{align}
	\frac{\partial H(\b{W})}{\partial \omega_{k,d}} =
		\sum_{n=1}^{N} \bracket{ P_k^{(n)} - t_k^{(n)} } \phi_{d,n} \label{eq:2.2}
\end{align}
$$

と与えられることがわかる。

### 最急降下法の規則まとめ

微分が得られたので最急降下法で停留解を得ることが可能になる。
これまでの議論をまとめると、規則は以下で与えられる

$$
\begin{aligned}
	\omega_{k,d}^{(new)} 
		& = \omega_{k,d} + \eta \frac{\partial H}{\partial \omega_{k,d}} \\
		& = \omega_{k,d} + \eta \sum_{n=1}^{N} \bracket{ P_k^{(n)} - t_k^{(n)} } \phi_{d,n} \\
	P_k^{(n)} & = \frac{\expA{a_k^{(n)}}}{\sum_{d=1}^{K} \expA{a_d^{(n)}}} \\
	a_k^{(n)} & = \sum_{d = 1}^{D} \omega_{k,d} \phi_{d,n} \\
	\phi_{d,n} & = \phi_d(\b{x^{(n)}})
\end{aligned}
$$

またこのようにして求められた最適解 $ \omega_{d,k}^{\star} $ にを用いて $ \eqref{eq:0.1} $ ラベルの予測を行えることができる

### おまけ（二次の微小量）
ちなみに最急降下法ではなく二次の微小量を用いてニュートンラフソン法を使うことも可能である

$ \eqref{eq:2.2} $ をさらに $ \omega_{s,t} $ で微分すると

$$
\begin{aligned}
	\frac{\partial^2 H}{\partial \omega_{s,t} \partial \omega_{k,d}} 
		& = \sum_{n=1}^{N} \frac{\partial P_k^{(n)}}{\partial \omega_{s,t}} \phi_{d,n} \\
		& = \sum_{n=1}^{N} \sum_{l=1}^{K} \frac{\partial P_k^{(n)}}{\partial a_l^{(n)}} \dfrac{\partial a_l^{(n)}}{\partial \omega_{s,t}} \phi_{d,n} \\
		& = \sum_{n=1}^{N} \sum_{l=1}^{K} P_k^{(n)} \bracket{\delta_{k,l} - P_{l}^{(n)}} \delta_{s,l} \phi_{t,n} \phi_{d,n} \\
		& = \sum_{n=1}^{N} P_k^{(n)} \bracket{\delta_{k,s} - P_{s}^{(n)}} \phi_{t,n} \phi_{d,n} \\
\end{aligned}
$$

と得られるため

